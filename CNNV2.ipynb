{"cells":[{"cell_type":"markdown","source":["# *Training a model to predict the location of bounding boxes and type of asphalt distress in the UAPD Dataset*\n","This model will use a VGG16 backbone and a classification head as well as a bounding box regression head. It requests a GPU from colab (if available). I changed this to one bounding box per image to make it an easier problem. However, there are multiple bounding boxes (and distresses) for some images which may confuse the model. More capable models such as YOLO can be used in practice.\n","\n","https://github.com/tantantetetao/UAPD-Pavement-Distress-Dataset"],"metadata":{"id":"-f0CuwPGp6pV"}},{"cell_type":"markdown","source":["# Mount Drive and Import packages"],"metadata":{"id":"rxu-rDaBrBli"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5od9o9LeQ7c","executionInfo":{"status":"ok","timestamp":1686340280193,"user_tz":240,"elapsed":46438,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"}},"outputId":"00258564-8cb8-4a79-bd06-e3c6b6d78b3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","CurrentDir='/content/drive/My Drive/Pavement/ICTD/WorkshopCNN' #path of project\n","os.chdir(CurrentDir)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iHwgbJbHeWY_","executionInfo":{"status":"ok","timestamp":1686340285004,"user_tz":240,"elapsed":4813,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras import layers, models, optimizers\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.utils import plot_model\n","from keras import backend as K"]},{"cell_type":"markdown","source":["# Load the saved dataset\n","Loading the npy files and then encoding the labels, dividing the bounding boxes to normalize between 0 and 1, and splitting into train and test"],"metadata":{"id":"ALq5lF1LrGCI"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"QmqCRPaLeW8J","executionInfo":{"status":"ok","timestamp":1686340317128,"user_tz":240,"elapsed":32133,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6369fc1c-14bd-4487-ce72-d68566654eee"},"outputs":[{"output_type":"stream","name":"stdout","text":["(array(['Crack', 'Pothole', 'Repair'], dtype='<U7'), array([1994,   44, 1104]))\n"]}],"source":["IMG_H=512 #512 pixels longs\n","IMG_W=IMG_H #same as height\n","Num_Class=3 #crack, repair, pothole\n","savefiles=False #you can change this to true once you make a copy on your own drive to overcome permissions\n","# Load the saved data\n","image_data = np.load('data/npdata/image_data.npy') \n","\n","bounding_boxes = np.load('data/npdata/bounding_boxes.npy') / IMG_H\n","labels = np.load('data/npdata/labels.npy')\n","\n","# Perform one-hot encoding on the labels\n","label_encoder = LabelEncoder()\n","encoded_labels = label_encoder.fit_transform(labels)\n","one_hot_labels = to_categorical(encoded_labels, num_classes=Num_Class)\n","train_labels = to_categorical(encoded_labels, Num_Class)\n","print(np.unique(labels,return_counts=True))\n","# Split the data into training and testing sets\n","train_images, test_images, train_labels, test_labels, train_bboxes, test_bboxes = train_test_split(\n","    image_data, one_hot_labels, bounding_boxes, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","source":["# Defining a model \n","VGG16 back bone and two heads. One for classification, one for bounding box"],"metadata":{"id":"DxwAXEzYGIHY"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4165,"status":"ok","timestamp":1686334536275,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"},"user_tz":240},"id":"ZOzfgaFXfkfu","outputId":"04553f02-39c5-4bb3-cfa8-f97a5f06527f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512, 512, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," block1_conv1 (Conv2D)          (None, 512, 512, 64  1792        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," block1_conv2 (Conv2D)          (None, 512, 512, 64  36928       ['block1_conv1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," block1_pool (MaxPooling2D)     (None, 256, 256, 64  0           ['block1_conv2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," block2_conv1 (Conv2D)          (None, 256, 256, 12  73856       ['block1_pool[0][0]']            \n","                                8)                                                                \n","                                                                                                  \n"," block2_conv2 (Conv2D)          (None, 256, 256, 12  147584      ['block2_conv1[0][0]']           \n","                                8)                                                                \n","                                                                                                  \n"," block2_pool (MaxPooling2D)     (None, 128, 128, 12  0           ['block2_conv2[0][0]']           \n","                                8)                                                                \n","                                                                                                  \n"," block3_conv1 (Conv2D)          (None, 128, 128, 25  295168      ['block2_pool[0][0]']            \n","                                6)                                                                \n","                                                                                                  \n"," block3_conv2 (Conv2D)          (None, 128, 128, 25  590080      ['block3_conv1[0][0]']           \n","                                6)                                                                \n","                                                                                                  \n"," block3_conv3 (Conv2D)          (None, 128, 128, 25  590080      ['block3_conv2[0][0]']           \n","                                6)                                                                \n","                                                                                                  \n"," block3_pool (MaxPooling2D)     (None, 64, 64, 256)  0           ['block3_conv3[0][0]']           \n","                                                                                                  \n"," block4_conv1 (Conv2D)          (None, 64, 64, 512)  1180160     ['block3_pool[0][0]']            \n","                                                                                                  \n"," block4_conv2 (Conv2D)          (None, 64, 64, 512)  2359808     ['block4_conv1[0][0]']           \n","                                                                                                  \n"," block4_conv3 (Conv2D)          (None, 64, 64, 512)  2359808     ['block4_conv2[0][0]']           \n","                                                                                                  \n"," block4_pool (MaxPooling2D)     (None, 32, 32, 512)  0           ['block4_conv3[0][0]']           \n","                                                                                                  \n"," block5_conv1 (Conv2D)          (None, 32, 32, 512)  2359808     ['block4_pool[0][0]']            \n","                                                                                                  \n"," block5_conv2 (Conv2D)          (None, 32, 32, 512)  2359808     ['block5_conv1[0][0]']           \n","                                                                                                  \n"," block5_conv3 (Conv2D)          (None, 32, 32, 512)  2359808     ['block5_conv2[0][0]']           \n","                                                                                                  \n"," block5_pool (MaxPooling2D)     (None, 16, 16, 512)  0           ['block5_conv3[0][0]']           \n","                                                                                                  \n"," classification_head (Sequentia  (None, 3)           67373571    ['block5_pool[0][0]']            \n"," l)                                                                                               \n","                                                                                                  \n"," bbox_head (Sequential)         (None, 4)            2106308     ['block5_pool[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 84,194,567\n","Trainable params: 69,479,623\n","Non-trainable params: 14,714,944\n","__________________________________________________________________________________________________\n"]}],"source":["# Load the VGG16 model as the base model\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_H, IMG_W, 3))\n","base_model.trainable = False\n","\n","# Classification head\n","classification_head = models.Sequential(name=\"classification_head\")\n","classification_head.add(layers.Flatten())\n","classification_head.add(layers.Dense(512, activation='relu'))\n","classification_head.add(layers.Dense(512, activation='relu'))\n","classification_head.add(layers.Dense(Num_Class, activation='softmax'))\n","\n","# Bounding box regression head\n","bbox_head = models.Sequential(name=\"bbox_head\")\n","bbox_head.add(layers.Conv2D(128, (2, 2), activation='relu'))  # Convolutional layer\n","bbox_head.add(layers.BatchNormalization())  # BatchNormalization layer\n","bbox_head.add(layers.Flatten())\n","bbox_head.add(layers.Dense(64, activation='relu'))\n","bbox_head.add(layers.Dense(4, activation='sigmoid'))  # 4 values for (xmin, ymin, xmax, ymax)\n","\n","\n","# Connect the heads to the base model\n","classification_output = classification_head(base_model.output)\n","bbox_output = bbox_head(base_model.output)\n","\n","# Create the complete model\n","model = models.Model(inputs=base_model.input, outputs=[classification_output, bbox_output])\n","\n","#plot_model(model, to_file='model.png')\n","model.summary()"]},{"cell_type":"markdown","source":["Compile the model"],"metadata":{"id":"-RldJOvQGUDA"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"jz6arzJcfpLG","executionInfo":{"status":"ok","timestamp":1686334541309,"user_tz":240,"elapsed":1098,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"}}},"outputs":[],"source":["# Compile the model with appropriate loss functions\n","lossWeights = {\n","    \"classification_head\": 1.0,\n","    \"bbox_head\": 1.0\n","}\n","model.compile(optimizer='adam',\n","              loss={'classification_head': 'categorical_crossentropy', 'bbox_head':'mse'},\n","              metrics={'classification_head': 'accuracy'}, loss_weights=lossWeights)\n","\n","num_epochs = 500\n","\n","model_path = os.path.join(\"model/\", \"model.h5\") #4,5,6 are good at classification not at bounding box\n","csv_path = os.path.join(\"model/\", \"log.csv\")\n","\n","if savefiles:\n","  callbacks = [ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n","              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n","              CSVLogger(csv_path, append=True),\n","              EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)]\n","else:\n","  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=False)]\n"]},{"cell_type":"markdown","source":["Train the model"],"metadata":{"id":"awukeew_GWlC"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"elapsed":257698,"status":"error","timestamp":1686334818157,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"},"user_tz":240},"id":"FW-2LQEofuD3","outputId":"3115675a-1444-4b63-9290-e3e36d48b858"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/500\n","71/71 [==============================] - 129s 1s/step - loss: 8.8178 - classification_head_loss: 8.6484 - bbox_head_loss: 0.1694 - classification_head_accuracy: 0.6984 - val_loss: 2.1997 - val_classification_head_loss: 2.0147 - val_bbox_head_loss: 0.1851 - val_classification_head_accuracy: 0.7421\n","Epoch 2/500\n","71/71 [==============================] - 60s 849ms/step - loss: 0.5807 - classification_head_loss: 0.4333 - bbox_head_loss: 0.1474 - classification_head_accuracy: 0.9045 - val_loss: 1.0097 - val_classification_head_loss: 0.8561 - val_bbox_head_loss: 0.1537 - val_classification_head_accuracy: 0.8294\n","Epoch 3/500\n","71/71 [==============================] - 60s 844ms/step - loss: 0.3993 - classification_head_loss: 0.2944 - bbox_head_loss: 0.1049 - classification_head_accuracy: 0.9615 - val_loss: 1.8237 - val_classification_head_loss: 1.7362 - val_bbox_head_loss: 0.0875 - val_classification_head_accuracy: 0.8294\n","Epoch 4/500\n"," 8/71 [==>...........................] - ETA: 43s - loss: 0.1219 - classification_head_loss: 0.0449 - bbox_head_loss: 0.0769 - classification_head_accuracy: 0.9883"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-37a455a63ab1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_images, {\"classification_head\": train_labels, \"bbox_head\": train_bboxes},\n\u001b[0m\u001b[1;32m      2\u001b[0m                     epochs=num_epochs, validation_split=0.1, callbacks=callbacks)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["history = model.fit(train_images, {\"classification_head\": train_labels, \"bbox_head\": train_bboxes},\n","                    epochs=num_epochs, validation_split=0.1, callbacks=callbacks)"]},{"cell_type":"markdown","source":["# Inference\n","Make predictions over some test images. We are going to load an already trained model to save time. "],"metadata":{"id":"uE5PkyZaGaxg"}},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image, ImageDraw, ImageFont\n","from tensorflow import keras\n","model_path = os.path.join(\"model/\", \"finalmodel.h5\")\n","if 'modelf' not in globals():\n","    modelf = keras.models.load_model(model_path)\n","# Select four example images from the dataset\n","indices = [0,1,2,5,8,12,16]  # Choose the indices of the images you want to predict 5\n","indices = [1,2,13,14,19,35,7]  # Choose the indices of the images you want to predict 4,6,9,12 #test 1,2,13,19,35,7\n","\n","\n","#image_data_examples = image_data[indices]\n","#label_examples = labels[indices]\n","#bbox_examples = bounding_boxes[indices]\n","\n","image_data_examples = test_images[indices]\n","label_examples = test_labels[indices]\n","bbox_examples = test_bboxes[indices]\n","# Reshape the image data to match the expected input shape of the model\n","image_data_examples = np.reshape(image_data_examples, (len(indices), 512, 512, 3))\n","\n","# Make predictions on the images\n","class_predictions, bbox_predictions = modelf.predict(image_data_examples)\n","\n","# Get the predicted labels\n","predicted_label_indices = np.argmax(class_predictions, axis=1)\n","true_label_indices=np.argmax(label_examples, axis=1)\n","predicted_labels = label_encoder.inverse_transform(predicted_label_indices)\n","true_labels = label_encoder.inverse_transform(true_label_indices)\n","\n","# Get the predicted bounding boxes\n","predicted_boxes = bbox_predictions\n","\n","# Loop over the examples and display the images with predicted vs actual labels\n","for i in range(len(indices)):\n","    image_example = Image.fromarray(image_data_examples[i].astype(np.uint8))\n","    true_label = true_labels[i]\n","    true_bbox = bbox_examples[i]\n","    predicted_label = predicted_labels[i]\n","    predicted_bbox = predicted_boxes[i]\n","    text = f\"Predicted: {predicted_label}\\nActual: {true_label}\"\n","    \n","    # Draw the labels on the image\n","    font = ImageFont.truetype(\"font/arial.ttf\", 18)\n","    draw = ImageDraw.Draw(image_example)\n","    bbox = draw.textbbox((20, 20), text, font=font)\n","    draw.rectangle(bbox, fill=\"red\")\n","    draw.text((20, 20), text, font=font, fill=\"black\")\n","\n","    # Draw the predicted bounding box on the image\n","    xmin_pred, ymin_pred, xmax_pred, ymax_pred = predicted_bbox*512\n","    draw.rectangle([(xmin_pred, ymin_pred), (xmax_pred, ymax_pred)], outline=\"red\", width=2)\n","\n","    # Draw the actual bounding box on the image\n","    xmin_true, ymin_true, xmax_true, ymax_true = true_bbox*512\n","    draw.rectangle([(xmin_true, ymin_true), (xmax_true, ymax_true)], outline=\"green\", width=2)\n","\n","    # Display the image\n","    image_example.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1f_k5uol1GA9hp07imUdG9aciMKBsnVVd"},"id":"WOhI5bm4uT6L","executionInfo":{"status":"ok","timestamp":1686340385874,"user_tz":240,"elapsed":48975,"user":{"displayName":"Egemen Okte","userId":"05773821952504250534"}},"outputId":"f056eabb-2211-4fd5-f7f8-15293adf871b"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPbmX6h+PbfIbbvB/sXGA6R"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}